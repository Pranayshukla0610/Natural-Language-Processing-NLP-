{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "201093e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word tokenization\n",
    "sent1 = 'I am going to delhi'\n",
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18c7840c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to delhi',\n",
       " ' I will stay there for 3 days',\n",
       " \" Let's hope the trip to be great\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence tokenization\n",
    "sent2 = 'I am going to delhi. I will stay there for 3 days. Let\\'s hope the trip to be great'\n",
    "sent2.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53929c3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Problem with split function\n",
    "sent3 = 'I am going to delhi!'\n",
    "sent3.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "834b7549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where do think I should go? I have 3 day holiday']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent4 = 'Where do think I should go? I have 3 day holiday'\n",
    "sent4.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c690681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To improve the problem\n",
    "import re\n",
    "sent3 = 'I am going to delhi!'\n",
    "tokens = re.findall(\"['\\w']+\",sent3)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e88da9a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Loren Ipsum is simply dummy text of the printing and typesetting industry',\n",
       " \" \\nLorem Ipsum has been the industry's standard dummy text even since the 1500s.\\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\"Loren Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text even since the 1500s.\n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sentences = re.compile('[,!?]').split(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab19eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more improved thing is to use library\n",
    "# NLTK\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca73fec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'visit', 'delhi', '!']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = 'I am going to visit delhi!'\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4295de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Loren Ipsum is simply dummy text of the printing and typesetting industry?',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text even since the 1500s.\",\n",
       " 'when an unknown printer took a galley of type and scrambled it to make a type specimen book.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\"Loren Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text even since the 1500s.\n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d17edbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent5 = 'I have a Ph.D in A.I'\n",
    "sent6 = \"We're here to help! mail us at nks@gmail.com\"\n",
    "sent7 = \"A 5 km ride cost $10.50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69ec642b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4533210f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " \"'re\",\n",
       " 'here',\n",
       " 'to',\n",
       " 'help',\n",
       " '!',\n",
       " 'mail',\n",
       " 'us',\n",
       " 'at',\n",
       " 'nks',\n",
       " '@',\n",
       " 'gmail.com']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Problem with NLTK library, but better than previous 2 methods\n",
    "word_tokenize(sent6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da207cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', '5', 'km', 'ride', 'cost', '$', '10.50']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78077f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best tokenization library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2ef121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(sent5)\n",
    "doc2 = nlp(sent6)\n",
    "doc3 = nlp(sent7)\n",
    "doc4 = nlp(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aba3694c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "going\n",
      "to\n",
      "visit\n",
      "delhi\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in doc4:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a818ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "have\n",
      "a\n",
      "Ph\n",
      ".\n",
      "D\n",
      "in\n",
      "A.I\n"
     ]
    }
   ],
   "source": [
    "# Here NLTK was better\n",
    "for token in doc1:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "745544d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "'re\n",
      "here\n",
      "to\n",
      "help\n",
      "!\n",
      "mail\n",
      "us\n",
      "at\n",
      "nks@gmail.com\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7451ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "5\n",
      "km\n",
      "ride\n",
      "cost\n",
      "$\n",
      "10.50\n"
     ]
    }
   ],
   "source": [
    "for token in doc3:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caac3fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No technique is perfect to tokenize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
